{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* In the first three lessons we did a decent amount of custom work.\n",
    "* In this last and final lesson, we first go over the parts of HuggingFace that might be most useful.\n",
    "  * We will be working from here https://huggingface.co/transformers/usage.html\n",
    "* We will then look at named entity recognition in spaCy.\n",
    "* And finally we will have a couple of concluding thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Pipelines\n",
    "* They have these pipelines that allow you do things quickly\n",
    "* See https://huggingface.co/transformers/main_classes/pipelines.html#the-task-specific-pipelines for more information\n",
    "* The horse's mouth to see the pipelines available https://github.com/huggingface/transformers/blob/master/src/transformers/pipelines.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Pipeline\n",
    "You can create text from a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 230/230 [00:00<00:00, 73.4kB/s]\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The space aliens came to me to talk about their own race to whom they had come to enslave. Because they were aliens, they were more like friends, the only beings of intelligence that humans came to know. I knew that for much but it just so happened that at least one of them was a human. And not even that one could live in my life, only by way of the Force, at least not to have any sort of sense of morality or morality or self. I was also'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"The space aliens came to me to talk about\", \n",
    "                     max_length=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Pipeline\n",
    "* Works best on news articles. Many models were trained on the Daily Mail bullets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 899k/899k [00:08<00:00, 110kB/s] \n",
      "Downloading: 100%|██████████| 456k/456k [00:04<00:00, 106kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"This weekend, I was able to convince my youngest son, age 7, to branch beyond chicken nuggets to Orange Chicken. I don't know why, but refactoring code is a lot of fun. #OddlyS\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Contrary to instructions, you need to do as below as illustrated\n",
    "# https://github.com/huggingface/transformers/issues/4504\n",
    "#summarizer = pipeline(\"summarization\")\n",
    "summarizer = pipeline('summarization',\n",
    "                      model='bart-large-cnn', \n",
    "                      tokenizer='bart-large-cnn')\n",
    "\n",
    "text = \"\"\"\n",
    "Cats don't like to wrestle.\n",
    "It would be nice if we could abandon all of this inane talk about celebrities and go back to gossiping about people that we actually know.\n",
    "Amazing how worthless I would be if you sent me back in time 10,000 years. I could describe amazing technology but couldn't build any of it.\n",
    "I can't believe we still have pennies. They aren't even worth picking up off the ground.\n",
    "Sometimes you wake and wish the adventure dream you were having didn't have to end. I want video games with that much excitement and realism\n",
    "Why isn't there an app that lets me share photos with all my social media sites? Facebook, G+, Twitter. Oh yeah, #NoProfitInFreedom\n",
    "This weekend, I was able to convince my youngest son, age 7, to branch beyond chicken nuggets to Orange Chicken. #CulinaryVictory\n",
    "I've found that being a fair-weather sports fan is a real time saver.\n",
    "My son, age 7, was playing \"restaurant,\" and the first thing he did was set up security cameras. #ModernWorld\n",
    "I don't know why, but refactoring code is a lot of fun. #OddlySatisfying\n",
    "Happiness is working on your projects.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(text, max_length=50, min_length=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Pipeline\n",
    "* It only has a few languages set up at the moment.\n",
    "* Google translate gives \"Les chats n'aiment pas lutter.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 230/230 [00:00<00:00, 49.3kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Les chats n'aiment pas se battre.\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"Cats don't like to wrestle.\", max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 230/230 [00:00<00:00, 64.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-MISC', 'index': 5, 'score': 0.8857880234718323, 'word': 'Jane'},\n",
      " {'entity': 'I-MISC', 'index': 6, 'score': 0.957772970199585, 'word': 'Austin'},\n",
      " {'entity': 'I-ORG', 'index': 9, 'score': 0.9754951000213623, 'word': 'Google'},\n",
      " {'entity': 'I-ORG', 'index': 10, 'score': 0.6128684878349304, 'word': 'Doc'},\n",
      " {'entity': 'I-ORG', 'index': 11, 'score': 0.8213635683059692, 'word': '##s'},\n",
      " {'entity': 'I-MISC',\n",
      "  'index': 20,\n",
      "  'score': 0.9108381867408752,\n",
      "  'word': 'Austen'},\n",
      " {'entity': 'I-LOC',\n",
      "  'index': 27,\n",
      "  'score': 0.9868152737617493,\n",
      "  'word': 'Austin'},\n",
      " {'entity': 'I-LOC', 'index': 29, 'score': 0.9850055575370789, 'word': 'Texas'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"ner\")\n",
    "\n",
    "tweet = \"\"\"If you write \"Jane Austin\", Google Docs recommends a correction' \\\n",
    "        'to \"Austen\". If you write \"Austin, Texas\", it doesn't. Nice.\"\"\"\n",
    "\n",
    "ners = nlp(tweet)\n",
    "import pprint\n",
    "pprint.pprint(ners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Named Entity Recognition\n",
    "on tokens \n",
    "https://spacy.io/api/token\n",
    "and at doc level\n",
    "https://spacy.io/usage/linguistic-features#named-entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If O \n",
      "you O \n",
      "write O \n",
      "\" O \n",
      "Jane B PERSON\n",
      "Austin I PERSON\n",
      "\" O \n",
      ", O \n",
      "Google B PERSON\n",
      "Docs I PERSON\n",
      "recommends O \n",
      "a O \n",
      "correction O \n",
      "' O \n",
      "         O \n",
      "' O \n",
      "to O \n",
      "\" O \n",
      "Austen B WORK_OF_ART\n",
      "\" O \n",
      ". O \n",
      "If O \n",
      "you O \n",
      "write O \n",
      "\" O \n",
      "Austin B PERSON\n",
      ", O \n",
      "Texas B GPE\n",
      "\" O \n",
      ", O \n",
      "it O \n",
      "does O \n",
      "n't O \n",
      ". O \n",
      "Nice O \n",
      ". O \n",
      "****** start document entities *******\n",
      "Jane Austin 14 25 PERSON\n",
      "Google Docs 28 39 PERSON\n",
      "Austen 78 84 WORK_OF_ART\n",
      "Austin 101 107 PERSON\n",
      "Texas 109 114 GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "tweet = \"\"\"If you write \"Jane Austin\", Google Docs recommends a correction' \\\n",
    "        'to \"Austen\". If you write \"Austin, Texas\", it doesn't. Nice.\"\"\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # note not vector model of before\n",
    "tokens = nlp(tweet)\n",
    "for token in tokens:\n",
    "    print(token, token.ent_iob_, token.ent_type_)\n",
    "    \n",
    "print(\"****** start document entities *******\")\n",
    "\n",
    "doc = nlp(tweet)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your own named entity recognizer in spaCy\n",
    "* You might want to train your own named entity recognizer for your domain-specific entities. You can see how to do that here\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Three main ways to get practical value from NLP now:\n",
    "* Find similar documents with vectors\n",
    "* Classification by converting documents to vectors\n",
    "* Extracting named entities of interest\n",
    "\n",
    "## Other Tools\n",
    "We ended up covering HuggingFace and spaCy. Other tools you might want to look at\n",
    "* OpenNMT: good for translations and summarization https://opennmt.net/\n",
    "* AllenNLP: more research oriented https://allennlp.org/\n",
    "* Stanford NLP: (in Java) if you need to do constituency parsing (make parsing trees) https://nlp.stanford.edu/software/\n",
    "* fast.ai: I haven't used it but I've heard good things. See https://www.fast.ai/ and https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda6a86cbe610af4db3860115ffdb24f1cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
